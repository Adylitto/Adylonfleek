{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adylitto/Adylonfleek/blob/main/WhisperYouTube.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're looking at this on GitHub and new to Python Notebooks or Colab, click the Google Colab badge above üëÜ\n",
        "\n",
        "\n",
        "#**Creating YouTube transcripts with OpenAI's Whisper model**\n",
        "\n",
        "üì∫ Getting started video: https://youtu.be/kENRf82_RQs\n",
        "\n",
        "*Colab beginner notes:*\n",
        "<br>\n",
        "1. These files are being loaded on a virtual machine in the cloud. Nothing is being downloaded to your computer (except for the transcript when you click to download it.) When you close this session the instance will be erased.\n",
        "<br>\n",
        "2. The run button is visible when you move your mouse close to the left edge of the code block. It looks kind of like this: ‚ñ∂Ô∏è ...but round...and white on black...so nothing like this. You'll know it when you see it.\n",
        "\n",
        "###**Note: For faster performance set your runtime to \"GPU\"**\n",
        "*Click on \"Runtime\" in the menu and click \"Change runtime type\". Select \"GPU\".*\n",
        "\n",
        "\n",
        "**Step 1.** Follow the instructions in each block and select the options you want\n",
        "<br>\n",
        "**Step 2.** Get the url of the video you want to transcribe\n",
        "<br>\n",
        "**Step 3.** Refresh the folder on the left and download your transcript\n",
        "<br>\n",
        "**Step 4.** Go to your YouTube account and upload the transcript to the video it came from and use \"autosync.\"\n",
        "\n",
        "That's it!\n",
        "\n",
        "Have a question? Hit me up on Twitter:[ @AndrewMayne](https://twitter.com/andrewmayne)\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**What is this?**\n",
        "<br>\n",
        "This is a Python notebook that creates a transcript from a YouTube url using OpenAI's Whisper transcription model that you can then upload to YouTube using the autosync feature to create captions.\n",
        "<br>  \n",
        "**What is OpenAI's Whisper model?**\n",
        "<br>\n",
        "Whisper is an automatic speech recognition (ASR) neural net created by OpenAI that transcribes audio at close to human level.\n",
        "<br>\n",
        "<br>\n",
        "**Why use this?**\n",
        "<br>\n",
        "The quality of the OpenAI Whisper model is amazing (I am slightly biased, but seriously, check it out.) You can also use it to transcribe in other languages.\n",
        "<br>\n",
        "<br>\n",
        "**What do the different model sizes do?**\n",
        "<br>\n",
        "Each model size has an improvement in quality ‚Äì especially with different languages. I've found that for a YouTube video with clear speech, the base model works really well. If you see transcription errors, you can try a larger model.\n",
        "<br>\n",
        "<br>\n",
        "**Do I need timestamps?**\n",
        "<br>\n",
        "Nope. YouTube's autosync function will match the text to the spoken words and syncs up really well. All you need is each spoken sentence in a .txt file.\n",
        "<br>\n",
        "<br>\n",
        "**How do I do this?**\n",
        "<br>\n",
        "Just follow each step. If you've never used Colab of a Python notebook, don't panic. It's super easy and runs in the cloud.\n",
        "<br>\n",
        "<br>\n",
        "**Does this cost anything to use?**\n",
        "<br>\n",
        "Nope. You can use Colab for free and Whisper is an open source model.\n",
        "<br>\n",
        "<br>\n",
        "[Tips for creating a YouTube transcript file](https://support.google.com/youtube/answer/2734799?hl=en)\n",
        "<br>\n",
        "[Information on OpenAI's Whisper model](https://openai.com/blog/whisper/)\n",
        "<br>\n",
        "[OpenAI's Whisper GitHub page](https://github.com/openai/whisper)\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qvz5JoKjwKAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1. Click the start button in the upper left side of this block to load the necessary libraries\n",
        "\n",
        "You will need to run this every time you reload this notebook.\n",
        "\"\"\"\n",
        "\n",
        "!pip install youtube_dl\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg\n",
        "!pip install librosa\n",
        "\n",
        "import whisper\n",
        "import time\n",
        "import librosa\n",
        "import re\n",
        "import youtube_dl"
      ],
      "metadata": {
        "id": "j6svgIwL1a-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa7781e6-a941-4493-eab7-bafadce834d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube_dl\n",
            "  Downloading youtube_dl-2021.12.17-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Downloading youtube_dl-2021.12.17-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: youtube_dl\n",
            "Successfully installed youtube_dl-2021.12.17\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-oy_o54h1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-oy_o54h1\n",
            "  Resolved https://github.com/openai/whisper.git to commit 517a43ecd132a2089d85f4ebc044728a71d49f6e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.6.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
            "Collecting tiktoken (from openai-whisper==20240930)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton>=2->openai-whisper==20240930) (3.17.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803669 sha256=e98c05322ea1457669b61c68365f532725935c73065f678d88da179a526caa62\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-efm0_lny/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930 tiktoken-0.9.0\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,370 kB]\n",
            "Hit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:6 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,731 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,668 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,682 kB]\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,235 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,533 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,988 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,934 kB]\n",
            "Fetched 25.5 MB in 10s (2,610 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "31 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "2. Select the model you want to use.\n",
        "\n",
        "Base works really well so it's the default.\n",
        "\n",
        "(For multilingual, remove \".en\" from the model name.)\n",
        "\n",
        "Click the run button after you've made your choice (or left it at default.)\n",
        "\"\"\"\n",
        "\n",
        "# model = whisper.load_model(\"tiny.en\")\n",
        "model = whisper.load_model(\"base\")\n",
        "# model = whisper.load_model(\"small.en\")\n",
        "# model = whisper.load_model(\"medium.en\")\n",
        "# model = whisper.load_model(\"large\")"
      ],
      "metadata": {
        "id": "9oRA4UIe104O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb5205dd-0794-4399-94b1-debdf308e7ec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139M/139M [00:01<00:00, 109MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yt-dlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y48uentfKI_L",
        "outputId": "72cf00ae-d8bf-481f-c3ed-567531effd5e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2025.2.19-py3-none-any.whl.metadata (171 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/171.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m171.9/171.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2025.2.19-py3-none-any.whl (3.2 MB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m204.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2025.2.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install yt-dlp tqdm\n",
        "\n",
        "# If you haven't already installed these\n",
        "# !pip install git+https://github.com/openai/whisper.git\n",
        "# !sudo apt update && sudo apt install ffmpeg\n",
        "# !pip install librosa\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import concurrent.futures\n",
        "import whisper\n",
        "import librosa\n",
        "from tqdm.notebook import tqdm\n",
        "import yt_dlp\n",
        "\n",
        "# Configure logging\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"transcription.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load Whisper model\n",
        "def load_model(model_size=\"medium\", device=\"cuda\"):\n",
        "    \"\"\"Load Whisper model with specified size and device.\"\"\"\n",
        "    print(f\"Loading Whisper {model_size} model on {device}...\")\n",
        "    model = whisper.load_model(model_size, device=device)\n",
        "    print(\"Model loaded successfully!\")\n",
        "    return model\n",
        "\n",
        "def get_channel_videos(channel_url, max_videos=None):\n",
        "    \"\"\"Get all video URLs from a YouTube channel using yt-dlp.\"\"\"\n",
        "    ydl_opts = {\n",
        "        'extract_flat': True,\n",
        "        'skip_download': True,\n",
        "        'ignoreerrors': True,\n",
        "        'quiet': True,\n",
        "        'no_warnings': True,\n",
        "    }\n",
        "\n",
        "    video_urls = []\n",
        "    print(f\"Extracting videos from channel: {channel_url}\")\n",
        "\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        try:\n",
        "            result = ydl.extract_info(channel_url, download=False)\n",
        "\n",
        "            if result and 'entries' in result:\n",
        "                for entry in result['entries']:\n",
        "                    if entry:\n",
        "                        video_urls.append(f\"https://www.youtube.com/watch?v={entry['id']}\")\n",
        "                        if max_videos and len(video_urls) >= max_videos:\n",
        "                            break\n",
        "\n",
        "            print(f\"Found {len(video_urls)} videos\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting channel videos: {str(e)}\")\n",
        "\n",
        "    return video_urls\n",
        "\n",
        "def chunk_text(text, segments, max_chunk_size=1000, overlap=100):\n",
        "    \"\"\"\n",
        "    Split transcript into semantic chunks with overlapping text.\n",
        "    This improves vector retrieval by maintaining context across chunks.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    # First, create a mapping of start and end times for the transcript\n",
        "    time_map = []\n",
        "    for segment in segments:\n",
        "        segment_text = segment[\"text\"]\n",
        "        start_time = segment[\"start\"]\n",
        "        end_time = segment[\"end\"]\n",
        "        time_map.append({\n",
        "            \"text\": segment_text,\n",
        "            \"start\": start_time,\n",
        "            \"end\": end_time\n",
        "        })\n",
        "\n",
        "    # Now chunk the full text with overlap\n",
        "    full_text = text\n",
        "\n",
        "    # If the text is short enough, just return it as a single chunk\n",
        "    if len(full_text) <= max_chunk_size:\n",
        "        # Find the time range\n",
        "        start_time = time_map[0][\"start\"] if time_map else 0\n",
        "        end_time = time_map[-1][\"end\"] if time_map else 0\n",
        "\n",
        "        return [{\n",
        "            \"text\": full_text,\n",
        "            \"start_time\": start_time,\n",
        "            \"end_time\": end_time\n",
        "        }]\n",
        "\n",
        "    # Otherwise, split it into overlapping chunks\n",
        "    start_idx = 0\n",
        "\n",
        "    while start_idx < len(full_text):\n",
        "        # Find the end index for this chunk\n",
        "        end_idx = start_idx + max_chunk_size\n",
        "\n",
        "        # If we're at the end of the text, just use the rest\n",
        "        if end_idx >= len(full_text):\n",
        "            end_idx = len(full_text)\n",
        "        else:\n",
        "            # Try to find a good breaking point (period, question mark, etc.)\n",
        "            # Look for these punctuation marks within the last 20% of the chunk\n",
        "            breaking_point = end_idx\n",
        "            search_start = max(start_idx, end_idx - int(max_chunk_size * 0.2))\n",
        "\n",
        "            # Find the last sentence break in the search range\n",
        "            last_period = full_text.rfind(\". \", search_start, end_idx)\n",
        "            last_question = full_text.rfind(\"? \", search_start, end_idx)\n",
        "            last_exclamation = full_text.rfind(\"! \", search_start, end_idx)\n",
        "\n",
        "            # Use the latest of these breaking points\n",
        "            candidates = [p for p in [last_period, last_question, last_exclamation] if p != -1]\n",
        "            if candidates:\n",
        "                breaking_point = max(candidates) + 2  # +2 to include the punctuation and space\n",
        "\n",
        "            end_idx = breaking_point\n",
        "\n",
        "        chunk_text = full_text[start_idx:end_idx].strip()\n",
        "\n",
        "        # Find the time range that corresponds to this chunk\n",
        "        # This is approximate and could be improved\n",
        "        start_time = None\n",
        "        end_time = None\n",
        "\n",
        "        # Calculate the approximate position of this chunk in the overall text\n",
        "        chunk_start_ratio = start_idx / len(full_text)\n",
        "        chunk_end_ratio = end_idx / len(full_text)\n",
        "\n",
        "        # Find corresponding timestamps based on position ratios\n",
        "        if time_map:\n",
        "            # Interpolate to find approximate start and end times\n",
        "            start_time = time_map[int(chunk_start_ratio * len(time_map))][\"start\"]\n",
        "            end_time = time_map[min(int(chunk_end_ratio * len(time_map)), len(time_map) - 1)][\"end\"]\n",
        "\n",
        "        chunks.append({\n",
        "            \"text\": chunk_text,\n",
        "            \"start_time\": start_time if start_time is not None else 0,\n",
        "            \"end_time\": end_time if end_time is not None else 0\n",
        "        })\n",
        "\n",
        "        # Move the start index for the next chunk, accounting for overlap\n",
        "        start_idx = end_idx - overlap\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def download_and_transcribe(url, model, output_dir=\"transcripts\", chunk_size=1000, overlap=100):\n",
        "    \"\"\"Download and transcribe a single YouTube video with semantic chunking for vector databases.\"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Create a yt-dlp options dictionary\n",
        "    ydl_opts = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'postprocessors': [{\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'mp3',\n",
        "            'preferredquality': '192',\n",
        "        }],\n",
        "        'outtmpl': '%(title)s.%(ext)s',\n",
        "        'quiet': False,\n",
        "        'no_warnings': True,\n",
        "    }\n",
        "\n",
        "    # Download the video and extract the audio\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(url, download=True)\n",
        "            video_title = info.get('title', 'Unknown Title')\n",
        "            video_id = info.get('id', 'Unknown ID')\n",
        "            channel = info.get('channel', 'Unknown Channel')\n",
        "            upload_date = info.get('upload_date', 'Unknown Date')\n",
        "            duration = info.get('duration', 0)\n",
        "            file_path = ydl.prepare_filename(info)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading video: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    # Get correct file path after post-processing\n",
        "    file_path = file_path.replace('.webm', '.mp3')\n",
        "    file_path = file_path.replace('.m4a', '.mp3')\n",
        "\n",
        "    # Get the duration\n",
        "    audio_duration = librosa.get_duration(filename=file_path)\n",
        "    print(f\"Processing: {video_title}\")\n",
        "    print(f\"Video length: {audio_duration:.2f} seconds\")\n",
        "\n",
        "    # Transcribe with word-level timestamps\n",
        "    start = time.time()\n",
        "    result = model.transcribe(file_path, word_timestamps=True)\n",
        "    end = time.time()\n",
        "    seconds = end - start\n",
        "    print(f\"Transcription time: {seconds:.2f} seconds\")\n",
        "\n",
        "    # Create output file paths\n",
        "    safe_title = re.sub(r'[\\/:*?\"<>|]', '_', video_title)\n",
        "    base_name = os.path.join(output_dir, safe_title)\n",
        "    json_path = f\"{base_name}.json\"\n",
        "    txt_path = f\"{base_name}.txt\"\n",
        "    word_timing_path = f\"{base_name}_word_timestamps.txt\"\n",
        "    vector_db_path = f\"{base_name}_vector_chunks.json\"\n",
        "\n",
        "    # Save raw JSON results (with word timestamps)\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(result, f, indent=2)\n",
        "\n",
        "    # Save plain text transcript\n",
        "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(result[\"text\"])\n",
        "\n",
        "    # Save word-level timestamps in a readable format\n",
        "    with open(word_timing_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"Transcript with word-level timestamps for: {video_title}\\n\\n\")\n",
        "        for segment in result[\"segments\"]:\n",
        "            f.write(f\"[{format_time(segment['start'])} --> {format_time(segment['end'])}]\\n\")\n",
        "\n",
        "            # Write each word with its timestamp\n",
        "            for word in segment.get(\"words\", []):\n",
        "                word_text = word[\"word\"]\n",
        "                word_start = format_time(word[\"start\"])\n",
        "                word_end = format_time(word[\"end\"])\n",
        "                f.write(f\"{word_text} [{word_start}-{word_end}]\\n\")\n",
        "\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "    # Create semantic chunks optimized for vector database\n",
        "    chunks = chunk_text(result[\"text\"], result[\"segments\"], chunk_size, overlap)\n",
        "\n",
        "    # Add metadata to chunks for vector database\n",
        "    vector_chunks = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunk_id = f\"{video_id}_{i}\"\n",
        "        vector_chunks.append({\n",
        "            \"id\": chunk_id,\n",
        "            \"text\": chunk[\"text\"],\n",
        "            \"metadata\": {\n",
        "                \"video_id\": video_id,\n",
        "                \"video_title\": video_title,\n",
        "                \"channel\": channel,\n",
        "                \"upload_date\": upload_date,\n",
        "                \"start_time\": chunk[\"start_time\"],\n",
        "                \"end_time\": chunk[\"end_time\"],\n",
        "                \"duration\": duration,\n",
        "                \"url\": f\"{url}&t={int(chunk['start_time'])}\",\n",
        "                \"chunk_index\": i,\n",
        "                \"total_chunks\": len(chunks)\n",
        "            }\n",
        "        })\n",
        "\n",
        "    # Save vector database chunks\n",
        "    with open(vector_db_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(vector_chunks, f, indent=2)\n",
        "\n",
        "    # Clean up the audio file\n",
        "    try:\n",
        "        os.remove(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Couldn't delete temporary audio file: {str(e)}\")\n",
        "\n",
        "    print(f\"Saved transcripts to: {json_path}, {txt_path}, and {word_timing_path}\")\n",
        "    print(f\"Saved vector database chunks to: {vector_db_path}\")\n",
        "    return json_path, txt_path, word_timing_path, vector_db_path\n",
        "\n",
        "def format_time(seconds):\n",
        "    \"\"\"Format timestamp as MM:SS.mmm.\"\"\"\n",
        "    minutes = int(seconds / 60)\n",
        "    secs = seconds % 60\n",
        "    return f\"{minutes:02d}:{secs:06.3f}\"\n",
        "\n",
        "def process_video_worker(args):\n",
        "    \"\"\"Worker function for parallel processing.\"\"\"\n",
        "    url, model, output_dir, chunk_size, overlap = args\n",
        "    try:\n",
        "        return download_and_transcribe(url, model, output_dir, chunk_size, overlap)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {url}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def batch_process_parallel(video_urls, model, output_dir=\"transcripts\", max_workers=2, chunk_size=1000, overlap=100):\n",
        "    \"\"\"Process multiple videos in parallel.\"\"\"\n",
        "    print(f\"Processing {len(video_urls)} videos with {max_workers} parallel workers\")\n",
        "\n",
        "    results = []\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Create arguments for each worker\n",
        "        worker_args = [(url, model, output_dir, chunk_size, overlap) for url in video_urls]\n",
        "\n",
        "        # Process videos in parallel with progress bar\n",
        "        futures = [executor.submit(process_video_worker, arg) for arg in worker_args]\n",
        "\n",
        "        for i, future in enumerate(tqdm(concurrent.futures.as_completed(futures), total=len(futures))):\n",
        "            result = future.result()\n",
        "            if result:\n",
        "                json_path, txt_path, word_timing_path, vector_db_path = result\n",
        "                results.append({\n",
        "                    \"url\": video_urls[i],\n",
        "                    \"json_path\": json_path,\n",
        "                    \"txt_path\": txt_path,\n",
        "                    \"word_timing_path\": word_timing_path,\n",
        "                    \"vector_db_path\": vector_db_path,\n",
        "                    \"status\": \"success\"\n",
        "                })\n",
        "            else:\n",
        "                results.append({\n",
        "                    \"url\": video_urls[i],\n",
        "                    \"status\": \"error\"\n",
        "                })\n",
        "\n",
        "    # Save a summary of all processed videos\n",
        "    summary_path = os.path.join(output_dir, \"batch_summary.json\")\n",
        "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    print(f\"Completed batch processing. Summary saved to {summary_path}\")\n",
        "    return results\n",
        "\n",
        "# Run this cell to load the model (do this only once)\n",
        "model = load_model(\"medium\")  # Options: \"tiny\", \"base\", \"small\", \"medium\", \"large\"\n",
        "\n",
        "# Cell for processing a single video\n",
        "def process_single_video():\n",
        "    video_url = input(\"Enter YouTube video URL: \")\n",
        "    output_dir = input(\"Output directory (default: 'transcripts'): \") or \"transcripts\"\n",
        "\n",
        "    download_and_transcribe(video_url, model, output_dir)\n",
        "\n",
        "# Cell for processing a YouTube channel\n",
        "def process_channel():\n",
        "    channel_url = input(\"Enter YouTube channel URL: \")\n",
        "    max_videos_input = input(\"Maximum number of videos to process (leave blank for all): \")\n",
        "    max_videos = int(max_videos_input) if max_videos_input.strip() else None\n",
        "    output_dir = input(\"Output directory (default: 'transcripts'): \") or \"transcripts\"\n",
        "    max_workers_input = input(\"Number of parallel workers (default: 2, recommended max: 3 for T4): \") or \"2\"\n",
        "    max_workers = int(max_workers_input)\n",
        "\n",
        "    video_urls = get_channel_videos(channel_url, max_videos)\n",
        "    batch_process_parallel(video_urls, model, output_dir, max_workers)\n",
        "\n",
        "# Cell for processing videos from a text file\n",
        "def process_from_file():\n",
        "    file_path = input(\"Enter the path to the text file with URLs (one per line): \")\n",
        "    output_dir = input(\"Output directory (default: 'transcripts'): \") or \"transcripts\"\n",
        "    max_workers_input = input(\"Number of parallel workers (default: 2, recommended max: 3 for T4): \") or \"2\"\n",
        "    max_workers = int(max_workers_input)\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        urls = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    batch_process_parallel(urls, model, output_dir, max_workers)\n",
        "\n",
        "# Simple menu for Colab\n",
        "def show_menu():\n",
        "    print(\"\\nYouTube Batch Transcription with Vector DB Output\")\n",
        "    print(\"-\" * 50)\n",
        "    print(\"1. Process a single video\")\n",
        "    print(\"2. Process a YouTube channel\")\n",
        "    print(\"3. Process videos from a text file\")\n",
        "    print(\"4. Exit\")\n",
        "\n",
        "    choice = input(\"\\nEnter your choice (1-4): \")\n",
        "\n",
        "    if choice == \"1\":\n",
        "        process_single_video()\n",
        "    elif choice == \"2\":\n",
        "        process_channel()\n",
        "    elif choice == \"3\":\n",
        "        process_from_file()\n",
        "    elif choice == \"4\":\n",
        "        print(\"Exiting...\")\n",
        "    else:\n",
        "        print(\"Invalid choice. Please try again.\")\n",
        "        show_menu()\n",
        "\n",
        "# Run this cell to show the menu\n",
        "show_menu()"
      ],
      "metadata": {
        "id": "JmbHC2-S33Kl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf0dd6ba-1e35-4a8c-c5c4-d2b04fee2b83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.11/dist-packages (2025.2.19)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Loading Whisper medium model on cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            "\n",
            "YouTube Batch Transcription with Vector DB Output\n",
            "--------------------------------------------------\n",
            "1. Process a single video\n",
            "2. Process a YouTube channel\n",
            "3. Process videos from a text file\n",
            "4. Exit\n",
            "\n",
            "Enter your choice (1-4): 1\n",
            "Enter YouTube video URL: https://www.youtube.com/watch?v=7tPI3PdJhwQ\n",
            "Output directory (default: 'transcripts'): \n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=7tPI3PdJhwQ\n",
            "[youtube] 7tPI3PdJhwQ: Downloading webpage\n",
            "[youtube] 7tPI3PdJhwQ: Downloading tv client config\n",
            "[youtube] 7tPI3PdJhwQ: Downloading player 91201489\n",
            "[youtube] 7tPI3PdJhwQ: Downloading tv player API JSON\n",
            "[youtube] 7tPI3PdJhwQ: Downloading ios player API JSON\n",
            "[youtube] 7tPI3PdJhwQ: Downloading m3u8 information\n",
            "[info] 7tPI3PdJhwQ: Downloading 1 format(s): 251\n",
            "[download] Destination: ÔºäLIVEÔºä Comment √©tablir une v√©ritable DISCIPLINE Ôºü.webm\n",
            "[download] 100% of   65.06MiB in 00:00:14 at 4.55MiB/s   \n",
            "[ExtractAudio] Destination: ÔºäLIVEÔºä Comment √©tablir une v√©ritable DISCIPLINE Ôºü.mp3\n"
          ]
        }
      ]
    }
  ]
}